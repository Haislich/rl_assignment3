{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial solution size: 867\n",
      "(5_w,11)-aCMA-ES (mu_w=3.4,w_1=42%) in dimension 867 (seed=645381, Fri Dec  6 17:02:22 2024)\n",
      "Iteration: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as T\n",
    "from IPython.display import display, HTML\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from rollout_dataset import RolloutDataset,RolloutDataloader,Episode\n",
    "from latent_dataset import LatentDataset,LatentDataloader,LatentEpisode\n",
    "\n",
    "from vision import ConvVAE,VisionTrainer\n",
    "from memory import MDN_RNN,MemoryTrainer\n",
    "from controller import Controller, ControllerTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_gif(\n",
    "    episode: Episode,\n",
    "    save_path=Path(\"media/rollout_dataset.gif\"),\n",
    "):\n",
    "    observations = episode.observations.unsqueeze(0).to(DEVICE)\n",
    "    scale_factor = 1\n",
    "    spacing = 1\n",
    "    img_width, img_height = 64 * scale_factor, 64 * scale_factor\n",
    "    total_width = img_width \n",
    "    total_height = img_height\n",
    "    images = []\n",
    "    for t in range(observations.shape[1]):\n",
    "        original_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(observations[0, t].cpu())\n",
    "        )\n",
    "        combined_img = Image.new(\"RGB\", (total_width, total_height), (0, 0, 0))\n",
    "        combined_img.paste(original_img, (0, 0))\n",
    "        images.append(combined_img)\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Save as GIF\n",
    "    images[0].save(\n",
    "        save_path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=200,  # Increase duration for slower playback\n",
    "        loop=0,\n",
    "    )\n",
    "    print(f\"Dataset GIF saved to {save_path}\")\n",
    "def create_vision_gif(\n",
    "    episode: Episode,\n",
    "    vision: ConvVAE,\n",
    "    save_path=Path(\"media/vae_reconstruction.gif\"),\n",
    "):\n",
    "    observations = episode.observations.unsqueeze(0).to(DEVICE)\n",
    "    latents = vision.get_batched_latents(observations)\n",
    "    vae_reconstructions = vision.decoder(latents.squeeze(0))\n",
    "    scale_factor = 1\n",
    "    spacing = 1\n",
    "    img_width, img_height = 64 * scale_factor, 64 * scale_factor\n",
    "    total_width = img_width * 2 + spacing * 2\n",
    "    total_height = img_height\n",
    "\n",
    "    images = []\n",
    "    for t in range(vae_reconstructions.shape[0]):\n",
    "        original_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(observations[0, t].cpu())\n",
    "        )\n",
    "        vae_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(vae_reconstructions[t].cpu())\n",
    "        )\n",
    "        combined_img = Image.new(\"RGB\", (total_width, total_height), (0, 0, 0))\n",
    "        combined_img.paste(original_img, (0, 0))\n",
    "        combined_img.paste(vae_img, (img_width + spacing, 0))\n",
    "        images.append(combined_img)\n",
    "\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    # Save as GIF\n",
    "    images[0].save(\n",
    "        save_path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=60,  # Increase duration for slower playback\n",
    "        loop=0,\n",
    "    )\n",
    "    print(f\"Vae reconstruction GIF saved to {save_path}\")\n",
    "def create_memory_gif(\n",
    "    episode: Episode,\n",
    "    vision: ConvVAE,\n",
    "    memory: MDN_RNN,\n",
    "    save_path=Path(\"media/vision_memory_reconstruction.gif\"),\n",
    "    display_gif_in_notebook=False,\n",
    "):\n",
    "    observations = episode.observations.unsqueeze(0).to(DEVICE)\n",
    "    actions = episode.actions.unsqueeze(0).to(DEVICE)\n",
    "\n",
    "    # Get latent representations from VAE\n",
    "    latents = vision.get_batched_latents(observations)\n",
    "\n",
    "    # Initialize RNN hidden state\n",
    "    hidden_state, cell_state = memory.init_hidden()\n",
    "    hidden_state = hidden_state.to(DEVICE)\n",
    "    cell_state = cell_state.to(DEVICE)\n",
    "\n",
    "    # Generate predictions using MDN-RNN\n",
    "    predicted_latents = []\n",
    "    for t in range(latents.shape[1] - 1):\n",
    "        pi, mu, sigma, hidden_state, cell_state = memory(\n",
    "            latents[:, t, :], actions[:, t, :], None, None\n",
    "        )\n",
    "        predicted_latent = memory.sample_latent(pi, mu, sigma)\n",
    "        predicted_latents.append(predicted_latent)\n",
    "\n",
    "    predicted_latents = torch.stack(predicted_latents, dim=1)\n",
    "\n",
    "    # Decode the latents\n",
    "    vae_reconstructions = vision.decoder(latents.squeeze(0))\n",
    "    memory_reconstructions = vision.decoder(predicted_latents.squeeze(0))\n",
    "\n",
    "    # Set up visualization parameters\n",
    "    scale_factor = 1\n",
    "    spacing = 1\n",
    "    img_width, img_height = 64 * scale_factor, 64 * scale_factor\n",
    "    total_width = img_width * 3 + spacing * 3\n",
    "    total_height = img_height\n",
    "\n",
    "    images = []\n",
    "\n",
    "    for t in range(vae_reconstructions.shape[0] - 1):\n",
    "        original_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(observations[0, t].cpu())\n",
    "        )\n",
    "        vision_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(vae_reconstructions[t].cpu())\n",
    "        )\n",
    "        memory_img = T.Resize((img_height, img_width))(\n",
    "            T.ToPILImage()(memory_reconstructions[t].cpu())\n",
    "        )\n",
    "\n",
    "        combined_img = Image.new(\"RGB\", (total_width, total_height), (0, 0, 0))\n",
    "        combined_img.paste(original_img, (0, 0))\n",
    "        combined_img.paste(vision_img, (img_width + spacing, 0))\n",
    "        combined_img.paste(memory_img, (2 * (img_width + spacing), 0))\n",
    "        images.append(combined_img)\n",
    "\n",
    "    save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save as GIF\n",
    "    images[0].save(\n",
    "        save_path,\n",
    "        save_all=True,\n",
    "        append_images=images[1:],\n",
    "        duration=60,\n",
    "        loop=0,\n",
    "    )\n",
    "    print(f\"VAE and Memory reconstruction GIF saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RolloutDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m rollout_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mRolloutDataset\u001b[49m(\n\u001b[1;32m      2\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m         num_rollouts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m,\n\u001b[1;32m      4\u001b[0m         max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[1;32m      5\u001b[0m     )\n\u001b[1;32m      6\u001b[0m (\n\u001b[1;32m      7\u001b[0m     train_episodes,\n\u001b[1;32m      8\u001b[0m     test_episodes,\n\u001b[1;32m      9\u001b[0m     val_episodes,\n\u001b[1;32m     10\u001b[0m ) \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mrandom_split(rollout_dataset, [\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.2\u001b[39m])\n\u001b[1;32m     11\u001b[0m training_dataset \u001b[38;5;241m=\u001b[39m RolloutDataset(\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     episodes\u001b[38;5;241m=\u001b[39m[rollout_dataset\u001b[38;5;241m.\u001b[39mepisodes_paths[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m train_episodes\u001b[38;5;241m.\u001b[39mindices],\n\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RolloutDataset' is not defined"
     ]
    }
   ],
   "source": [
    "rollout_dataset = RolloutDataset(\n",
    "        \"create\",\n",
    "        num_rollouts=5000,\n",
    "        max_steps=500,\n",
    "    )\n",
    "(\n",
    "    train_episodes,\n",
    "    test_episodes,\n",
    "    val_episodes,\n",
    ") = torch.utils.data.random_split(rollout_dataset, [0.5, 0.3, 0.2])\n",
    "training_dataset = RolloutDataset(\n",
    "    \"from\",\n",
    "    episodes=[rollout_dataset.episodes_paths[idx] for idx in train_episodes.indices],\n",
    ")\n",
    "test_dataset = RolloutDataset(\n",
    "    \"from\",\n",
    "    episodes=[rollout_dataset.episodes_paths[idx] for idx in test_episodes.indices],\n",
    ")\n",
    "val_dataset = RolloutDataset(\n",
    "    \"from\",\n",
    "    episodes=[rollout_dataset.episodes_paths[idx] for idx in val_episodes.indices],\n",
    ")\n",
    "train_dataloader = RolloutDataloader(training_dataset, 64)\n",
    "test_dataloader = RolloutDataloader(test_dataset, 64)\n",
    "val_dataloader = RolloutDataloader(val_dataset, 64)\n",
    "episode = rollout_dataset[0]\n",
    "create_dataset_gif(episode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 | Train Loss: 1.8611 | Test Loss: 3.0302\n",
      "Model saved to models/vision.pt\n",
      "Vae reconstruction GIF saved to vae_reconstruction.gif\n"
     ]
    }
   ],
   "source": [
    "vision = ConvVAE().to(DEVICE)\n",
    "vision_trainer = VisionTrainer()\n",
    "vision_trainer.train(\n",
    "    vision,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    torch.optim.Adam(vision.parameters()),\n",
    "    epochs=1,\n",
    ")\n",
    "episode = rollout_dataset[0]\n",
    "create_vision_gif(episode, vision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_training_set = LatentDataset(\n",
    "        training_dataset,\n",
    "        vision,\n",
    "        \"create\",\n",
    "    )\n",
    "latent_test_set = LatentDataset(\n",
    "    test_dataset,\n",
    "    vision,\n",
    "    \"create\",\n",
    "    )\n",
    "latent_val_set = LatentDataset(\n",
    "    val_dataset,\n",
    "    vision,\n",
    "    \"create\",\n",
    ")\n",
    "\n",
    "train_dataloader = LatentDataloader(latent_training_set, 64)\n",
    "test_dataloader = LatentDataloader(latent_test_set, 64)\n",
    "test_dataloader = LatentDataloader(latent_val_set, 64)\n",
    "memory = MDN_RNN().to(DEVICE)\n",
    "memory_trainer = MemoryTrainer()\n",
    "memory_trainer.train(\n",
    "    memory,\n",
    "    train_dataloader,\n",
    "    test_dataloader,\n",
    "    torch.optim.Adam(memory.parameters()),\n",
    "    save_path=Path(\"models/memory_continuos.pt\"),\n",
    ")\n",
    "create_memory_gif(episode, vision,memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = ConvVAE.from_pretrained().to(\"cpu\")\n",
    "memory = MDN_RNN.from_pretrained().to(\"cpu\")\n",
    "controller = Controller().to(\"cpu\")\n",
    "controller_trainer = ControllerTrainer(controller, vision, memory, population_size=11)\n",
    "controller_trainer.train(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
